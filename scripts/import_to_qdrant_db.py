import os
import glob
import math
from lxml import etree
from qdrant_client import QdrantClient, models
# å¯¼å…¥ SentenceTransformer åº“
from sentence_transformers import SentenceTransformer

# --- é…ç½® ---
COLLECTION_NAME = "pubmed" # é›†åˆåç§°ç•¥ä½œä¿®æ”¹ä»¥åŒºåˆ†
QDRANT_HOST = "localhost" # æ›¿æ¢ä¸ºæ‚¨çš„ Qdrant æœåŠ¡åœ°å€
QDRANT_PORT = 6333
# SentenceTransformer å°†ç›´æ¥ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœæœ¬åœ°æ²¡æœ‰ç¼“å­˜ï¼‰
MODEL_NAME = "BAAI/bge-m3" 
XML_ROOT_DIR = "pubmed_baseline/xml" # æ‚¨çš„ XML æ–‡ä»¶æ ¹ç›®å½•
BATCH_FILE_SIZE = 10         # æ¯æ‰¹æ¬¡å¤„ç†çš„æ–‡ä»¶æ•°é‡

BGE_M3_DIMENSION = 1024
# å®šä¹‰ PubMed XML çš„å¸¸ç”¨å‘½åç©ºé—´
NS = {'pubmed': 'http://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_250101.dtd'}

# --- æ ¸å¿ƒå‡½æ•° (XML è§£æ) ---

def parse_pubmed_xml(xml_file_path):
    """è§£æå•ä¸ª PubMed XML æ–‡ä»¶ï¼Œè¿”å›æ–‡ç« åˆ—è¡¨ã€‚"""
    try:
        # ä½¿ç”¨ lxml è§£æï¼Œå®ƒå¯ä»¥å¤„ç†å¤§å‹æ–‡ä»¶å’Œå‘½åç©ºé—´
        tree = etree.parse(xml_file_path)
        root = tree.getroot()
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶ {xml_file_path}: {e}")
        return []
        
    articles = []
    
    for article_node in root.findall('.//PubmedArticle'):
        try:
            pmid = article_node.xpath('.//PMID/text()')[0]
            
            title_node = article_node.xpath('.//ArticleTitle')
            title = title_node[0].text.strip() if title_node and title_node[0].text else "No Title"

            abstract_parts = article_node.xpath('.//AbstractText/text()')
            abstract = ' '.join(part.strip() for part in abstract_parts if part).strip()
            
            authors = []
            for author in article_node.xpath('.//AuthorList/Author'):
                last_name = author.xpath('./LastName/text()')
                fore_names = author.xpath('./ForeName/text()')
                if last_name and fore_names:
                    authors.append(f"{last_name[0]}, {fore_names[0]}")
                elif last_name:
                    authors.append(last_name[0])

            date_revised_parts = article_node.xpath('.//DateRevised')
            update_date = None
            if date_revised_parts:
                date_node = date_revised_parts[0]
                year = date_node.xpath('./Year/text()')[0]
                month = date_node.xpath('./Month/text()')[0]
                day = date_node.xpath('./Day/text()')[0]
                update_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"

            journal_title_node = article_node.xpath('.//Journal/Title/text()')
            journal_title = journal_title_node[0] if journal_title_node else None

            # åµŒå…¥æºæ–‡æœ¬ (Title + Abstract)
            embedding_text = f"Title: {title}. Abstract: {abstract}"

            articles.append({
                'pmid': pmid,
                'title': title,
                'abstract': abstract,
                'authors': authors,
                'update_date': update_date,
                'journal_title': journal_title,
                'embedding_text': embedding_text,
            })
            
        except Exception:
            # å¿½ç•¥è§£æå¤±è´¥çš„æ–‡ç« 
            continue
            
    print(f"âœ… æ–‡ä»¶ {os.path.basename(xml_file_path)} è§£æå®Œæˆï¼Œæå– {len(articles)} ç¯‡æ–‡ç« ã€‚")
    return articles


def ingest_to_qdrant(articles, model, client):
    """ç”ŸæˆåµŒå…¥å¹¶æ‰¹é‡ä¸Šä¼ åˆ° Qdrantã€‚"""
    
    embedding_texts = [art['embedding_text'] for art in articles]
    
    print(f"ğŸ§  æ­£åœ¨ç”Ÿæˆ {len(articles)} ç¯‡æ–‡ç« çš„ SentenceTransformer åµŒå…¥...")
    
    # --- å…³é”®æ›´æ”¹ï¼šä½¿ç”¨ SentenceTransformer.encode ---
    # BGE-M3 æ¨¡å‹æ˜¯ SentenceTransformer å…¼å®¹çš„ï¼Œæ‰€ä»¥ç›´æ¥è°ƒç”¨ encode å³å¯
    dense_vectors = model.encode(
        sentences=embedding_texts, 
        batch_size=32,
        normalize_embeddings=True, # å»ºè®®å¯ç”¨å½’ä¸€åŒ–ï¼Œè¿™å¯¹å‘é‡æ£€ç´¢éå¸¸å…³é”®
        show_progress_bar=False
    ).tolist()
    
    points = []
    for i, article in enumerate(articles):
        payload = {
            "pmid": article['pmid'],
            "title": article['title'],
            "abstract": article['abstract'],
            "authors": article['authors'],
            "update_date": article['update_date'],
            "journal_title": article['journal_title'],
        }

        # æ„é€  PointStruct
        try:
            point_id = int(article['pmid'])
        except ValueError:
            point_id = article['pmid']

        points.append(models.PointStruct(
            id=point_id, 
            vector=dense_vectors[i],
            payload=payload
        ))

    # æ‰¹é‡ä¸Šä¼ 
    print(f"\nâ¬†ï¸ æ­£åœ¨æ‰¹é‡ä¸Šä¼  {len(points)} ä¸ª Points åˆ° Qdrant...")
    client.upsert(
        collection_name=COLLECTION_NAME,
        wait=True,
        points=points,
        batch_size=128
    )

    print("âœ… æ•°æ®å…¥åº“å®Œæˆï¼")


def main(payload:dict = None):
    if not os.path.isdir(XML_ROOT_DIR):
        print(f"âŒ é”™è¯¯ï¼šXML æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨æˆ–è·¯å¾„ä¸æ­£ç¡®: {XML_ROOT_DIR}")
        exit()

    try:
        print(f"ğŸ§  æ­£åœ¨åŠ è½½ SentenceTransformer æ¨¡å‹: {MODEL_NAME}")
        # SentenceTransformer ä¼šè‡ªåŠ¨æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼Œå¦‚æœ D:\bge-m3 å·²ç»ä¸‹è½½å¥½äº†ï¼Œ
        # å®ƒä¼šä¼˜å…ˆä»ç¼“å­˜åŠ è½½ã€‚å¦åˆ™å®ƒä¼šä» Hugging Face Hub ä¸‹è½½ã€‚
        model = SentenceTransformer(MODEL_NAME)
        print("ğŸ‰ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        
        # ç¡®ä¿é›†åˆå­˜åœ¨ (ä»…åœ¨å¼€å§‹æ—¶åˆ›å»ºä¸€æ¬¡)
        print(f"\nâš™ï¸ æ£€æŸ¥/åˆ›å»º Qdrant é›†åˆ: {COLLECTION_NAME}")
        client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(size=BGE_M3_DIMENSION, distance=models.Distance.COSINE),
        )

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥ (æ¨¡å‹æˆ– Qdrant è¿æ¥): {e}")
        exit()
        
    # è·å–æ‰€æœ‰ XML æ–‡ä»¶
    # ä½¿ç”¨ os.path.join ç¡®ä¿è·¯å¾„åˆ†éš”ç¬¦åœ¨ä¸åŒç³»ç»Ÿä¸Šæ­£ç¡®
    all_xml_files = glob.glob(os.path.join(XML_ROOT_DIR, "*.xml"))
    total_files = len(all_xml_files)
    
    if total_files == 0:
        print(f"âš ï¸ ç›®å½• {XML_ROOT_DIR} ä¸­æœªæ‰¾åˆ°ä»»ä½• XML æ–‡ä»¶ã€‚")
        exit()

    total_batches = math.ceil(total_files / BATCH_FILE_SIZE)
    total_articles_ingested = 0

    # å¾ªç¯å¤„ç†æ–‡ä»¶æ‰¹æ¬¡ (æ¯ 10 ä¸ªæ–‡ä»¶ä¸€ç»„)
    for i in range(0, total_files, BATCH_FILE_SIZE):
        file_batch = all_xml_files[i:i + BATCH_FILE_SIZE]
        
        print(f"\n--- ğŸš€ æ­£åœ¨å¤„ç†æ–‡ä»¶æ‰¹æ¬¡ {i // BATCH_FILE_SIZE + 1}/{total_batches} (æ–‡ä»¶æ•°: {len(file_batch)}) ---")
        
        articles_to_ingest = []
        
        # 1. è§£æå’Œèšåˆæ‰€æœ‰æ–‡ä»¶ä¸­çš„æ–‡ç« 
        for file_path in file_batch:
            current_file_articles = parse_pubmed_xml(file_path)
            articles_to_ingest.extend(current_file_articles)
            
        print(f"èšåˆå®Œæˆï¼šæœ¬æ–‡ä»¶æ‰¹æ¬¡å…± {len(articles_to_ingest)} ç¯‡æ–‡ç« å‡†å¤‡å…¥åº“ã€‚")
        
        # 2. ç”ŸæˆåµŒå…¥å¹¶å…¥åº“
        if articles_to_ingest:
            ingest_to_qdrant(articles_to_ingest, model, client)
            total_articles_ingested += len(articles_to_ingest)
        else:
            print("è­¦å‘Šï¼šæœ¬æ‰¹æ¬¡æœªè§£æåˆ°æœ‰æ•ˆæ–‡ç« ï¼Œè·³è¿‡å…¥åº“ã€‚")

    print("\n--- ğŸ æ‰€æœ‰æ–‡ä»¶æ‰¹æ¬¡å¤„ç†å®Œæˆ ---")
    final_count = client.count(collection_name=COLLECTION_NAME, exact=True).count
    print(f"æœ€ç»ˆ Qdrant é›†åˆ '{COLLECTION_NAME}' ä¸­åŒ…å« {final_count} ä¸ªå‘é‡ï¼Œå…±å¤„ç†æ–‡ç«  {total_articles_ingested} ç¯‡ã€‚")
    
    
# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
if __name__ == "__main__":
    main()